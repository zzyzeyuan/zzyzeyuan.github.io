---
title: 'Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs(EGT)'
date: 2023-06-02
# permalink: /posts/2012/08/blog-post-4/
permalink: /posts/2023/06/02/EGT/
# tags:
#   - cool posts
#   - category1
#   - category2
---

## Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs [SIGKDD2022]

<img src="https://p.ipic.vip/nng7bg.png" alt="image-20230612201436826" style="zoom:50%;" />

-   EGT是直接对图应用全局自注意力
-   可以直接接受、处理和输出结构信息以及节点信息
-   位置编码是基于奇异值分解的图的广义编码方案

<img src="https://p.ipic.vip/7hfxl7.png" alt="image-20230613145539336" style="zoom:50%;" />

第一个式子将邻接矩阵进行SVD分解，再拆解成 $\hat{U}, \hat{V}$从而得到$\hat{\Gamma}$ ，$\hat{\Gamma}$的每一行包含了edge去噪(denoised)的信息，所以能用作位置编码。($\hat{U}的第i行,\hat{V}的第j行$的点积可以近似邻接矩阵$A_{ij}$)。*【不是很懂为什么就包含了edge去噪以后的信息】*

